{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: /SSD2/chenzx/miniconda3/envs/fcgs/lib/python3.11/site-packages/lpips/weights/v0.1/vgg.pth\n",
      "[ INFO ] Encountered quite large input images (>1.6K pixels width), rescaling to 1.6K.\n",
      " If this is not desired, please explicitly specify '--resolution/-r' as 1\n",
      "Start decompressing xyz...\n",
      "Start decompressing fea...\n",
      "Start decompressing feq...\n",
      "Start decompressing geo...\n",
      "Decompressed ply file saved to /SDD_D/zwk/output/cook_spinach-3-ori/init_3dgs.ply!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rendering progress:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/SSD2/chenzx/Projects/FCGS/decode_single_scene_validate.py\", line 133, in <module>\n",
      "    train(args)\n",
      "  File \"/SSD2/chenzx/Projects/FCGS/decode_single_scene_validate.py\", line 103, in train\n",
      "    rendering = render(view, gaussians, pipe=pipeline, bg_color=torch.tensor([0, 0, 0], dtype=torch.float32, device=\"cuda\"))[\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/SSD2/chenzx/Projects/FCGS/gaussian_renderer/__init__.py\", line 102, in render\n",
      "    \"visibility_filter\" : radii > 0,\n",
      "                          ^^^^^^^^^\n",
      "RuntimeError: CUDA error: an illegal memory access was encountered\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "lmbda_list = [4e-4]\n",
    "scene = 'cook_spinach-3-ori'\n",
    "path_to_ply = f'/SDD_D/zwk/output/{scene}/additional_3dgs'\n",
    "path_to_save = f'/SDD_D/zwk/output/{scene}/additional_3dgs_c'\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "for lmbda in lmbda_list:\n",
    "    # for ply in os.listdir(path_to_ply):\n",
    "    #     os.system(f'python encode_single_scene.py --gpu 3 --lmd {lmbda} --ply_path_from {path_to_ply}/{ply} --bit_path_to {path_to_save}/{ply.split(\".\")[0]} --determ 1')\n",
    "    # os.system(f'python encode_single_scene.py --gpu 3 --lmd {lmbda} --ply_path_from {path_to_ply} --bit_path_to {path_to_save} --determ 1')\n",
    "    # os.system(f'python decode_single_scene_validate.py --gpu 0 --lmd {lmbda} --bit_path_from ./outputs/{exp_name}/{scene} --ply_path_to {path_to_ply}/{scene}/point_cloud/iteration_1000/point_cloud.ply --source_path ../data_static/mipnerf360/{scene}')\n",
    "\n",
    "    # os.system(f'python encode_single_scene.py --gpu 3 --lmd {lmbda} --ply_path_from \"/SDD_D/zwk/output/{scene}/init_3dgs.ply\" --bit_path_to \"/SDD_D/zwk/output/{scene}/init_3dgs_c\" --determ 1')\n",
    "    os.system(f'python -W ignore decode_single_scene_validate.py --gpu 0 --lmd {lmbda} --bit_path_from /SDD_D/zwk/output/{scene}/init_3dgs_c --ply_path_to /SDD_D/zwk/output/{scene}/init_3dgs.ply --source_path /SDD_D/zwk/data_dynamic/dynerf/{scene.split(\"-\")[0]}/frame000000')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system(\"export CUDA_LAUNCH_BLOCKING=1\")\n",
    "\n",
    "lmbda_list = [4e-4]\n",
    "scene = 'cook_spinach-3'\n",
    "ply_path = f'/SDD_D/zwk/output/{scene}/frame000005/point_cloud/iteration_150/motion/point_cloud.ply'\n",
    "ply_path_save = f'/SDD_D/zwk/output/{scene}/frame000005/point_cloud/iteration_150/motion_c'\n",
    "for lmbda in lmbda_list:\n",
    "    os.system(f'python encode_single_scene.py --gpu 0 --lmd {lmbda} --ply_path_from {ply_path} --bit_path_to {ply_path_save} --determ 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "lmbda_list = [16e-4, 8e-4, 4e-4]\n",
    "exp_name = '3DGS_ori_mip360'\n",
    "path_to_ply = '../gaussian-splatting/output/mipnerf360'\n",
    "scene_list = os.listdir(path_to_ply)\n",
    "for lmbda in lmbda_list:\n",
    "    for scene in ['flowers']:\n",
    "        print('-'*100,'\\n', scene)\n",
    "        os.system(f'python encode_single_scene.py --gpu 3 --lmd {lmbda} --ply_path_from {path_to_ply}/{scene}/point_cloud/iteration_30000/point_cloud.ply --bit_path_to ./outputs/{exp_name}/{scene} --determ 1')\n",
    "        os.system(f'python -W ignore decode_single_scene_validate.py --gpu 3 --lmd {lmbda} --bit_path_from ./outputs/{exp_name}/{scene} --ply_path_to {path_to_ply}/{scene}/point_cloud/iteration_30000/point_cloud.ply --source_path ../data_static/mipnerf360/{scene}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinycudann as tcnn\n",
    "import numpy as np\n",
    "import torch\n",
    "from model.grid_utils import normalize_xyz, _grid_creater, _grid_encoder, FreqEncoder\n",
    "\n",
    "encoding_cofig = {\n",
    "\t\"otype\": \"Frequency\", \n",
    "\t\"n_frequencies\": 4  \n",
    "}\n",
    "\n",
    "encoder = tcnn.Encoding(3, encoding_cofig)\n",
    "input = torch.rand(6, 3).cuda()\n",
    "output = encoder(input)\n",
    "print(input)\n",
    "print(output)\n",
    "\n",
    "encoder2 = FreqEncoder(3, 4)\n",
    "output2 = encoder2(input)\n",
    "print(output2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MotionMaskGenerator.0.weight torch.Size([64, 56])\n",
      "MotionMaskGenerator.0.bias torch.Size([64])\n",
      "MotionMaskGenerator.2.weight torch.Size([64, 64])\n",
      "MotionMaskGenerator.2.bias torch.Size([64])\n",
      "MotionMaskGenerator.4.weight torch.Size([1, 64])\n",
      "MotionMaskGenerator.4.bias torch.Size([1])\n",
      "FeatureExtractor.0.weight torch.Size([64, 56])\n",
      "FeatureExtractor.0.bias torch.Size([64])\n",
      "FeatureExtractor.1.beta torch.Size([64])\n",
      "FeatureExtractor.1.gamma torch.Size([64, 64])\n",
      "FeatureExtractor.3.weight torch.Size([64, 64])\n",
      "FeatureExtractor.3.bias torch.Size([64])\n",
      "FeatureExtractor.4.beta torch.Size([64])\n",
      "FeatureExtractor.4.gamma torch.Size([64, 64])\n",
      "FeatureExtractor.6.weight torch.Size([64, 64])\n",
      "FeatureExtractor.6.bias torch.Size([64])\n",
      "MotionEncoder.0.weight torch.Size([64, 7])\n",
      "MotionEncoder.0.bias torch.Size([64])\n",
      "MotionEncoder.1.beta torch.Size([64])\n",
      "MotionEncoder.1.gamma torch.Size([64, 64])\n",
      "MotionEncoder.3.weight torch.Size([64, 64])\n",
      "MotionEncoder.3.bias torch.Size([64])\n",
      "MotionEncoder.4.beta torch.Size([64])\n",
      "MotionEncoder.4.gamma torch.Size([64, 64])\n",
      "MotionEncoder.6.weight torch.Size([64, 64])\n",
      "MotionEncoder.6.bias torch.Size([64])\n",
      "MotionDecoder.0.weight torch.Size([64, 64])\n",
      "MotionDecoder.0.bias torch.Size([64])\n",
      "MotionDecoder.1.beta torch.Size([64])\n",
      "MotionDecoder.1.gamma torch.Size([64, 64])\n",
      "MotionDecoder.3.weight torch.Size([64, 64])\n",
      "MotionDecoder.3.bias torch.Size([64])\n",
      "MotionDecoder.4.beta torch.Size([64])\n",
      "MotionDecoder.4.gamma torch.Size([64, 64])\n",
      "MotionDecoder.6.weight torch.Size([7, 64])\n",
      "MotionDecoder.6.bias torch.Size([7])\n",
      "MotionPriorEncoder.0.weight torch.Size([64, 64])\n",
      "MotionPriorEncoder.0.bias torch.Size([64])\n",
      "MotionPriorEncoder.1.beta torch.Size([64])\n",
      "MotionPriorEncoder.1.gamma torch.Size([64, 64])\n",
      "MotionPriorEncoder.3.weight torch.Size([64, 64])\n",
      "MotionPriorEncoder.3.bias torch.Size([64])\n",
      "MotionPriorEncoder.4.beta torch.Size([64])\n",
      "MotionPriorEncoder.4.gamma torch.Size([64, 64])\n",
      "MotionPriorEncoder.6.weight torch.Size([64, 64])\n",
      "MotionPriorEncoder.6.bias torch.Size([64])\n",
      "MotionPriorDecoder.0.weight torch.Size([64, 64])\n",
      "MotionPriorDecoder.0.bias torch.Size([64])\n",
      "MotionPriorDecoder.1.beta torch.Size([64])\n",
      "MotionPriorDecoder.1.gamma torch.Size([64, 64])\n",
      "MotionPriorDecoder.3.weight torch.Size([64, 64])\n",
      "MotionPriorDecoder.3.bias torch.Size([64])\n",
      "MotionPriorDecoder.4.beta torch.Size([64])\n",
      "MotionPriorDecoder.4.gamma torch.Size([64, 64])\n",
      "MotionPriorDecoder.6.weight torch.Size([64, 64])\n",
      "MotionPriorDecoder.6.bias torch.Size([64])\n",
      "AutoRegressiveMotion.0.weight torch.Size([1, 1, 3])\n",
      "AutoRegressiveMotion.0.bias torch.Size([1])\n",
      "AutoRegressiveMotion.0.mask torch.Size([1, 1, 3])\n",
      "AutoRegressiveMotion.2.weight torch.Size([1, 1, 3])\n",
      "AutoRegressiveMotion.2.bias torch.Size([1])\n",
      "AutoRegressiveMotion.2.mask torch.Size([1, 1, 3])\n",
      "AutoRegressiveMotion.4.weight torch.Size([1, 1, 3])\n",
      "AutoRegressiveMotion.4.bias torch.Size([1])\n",
      "AutoRegressiveMotion.4.mask torch.Size([1, 1, 3])\n",
      "EntropyParametersMotion.0.weight torch.Size([64, 128])\n",
      "EntropyParametersMotion.0.bias torch.Size([64])\n",
      "EntropyParametersMotion.2.weight torch.Size([64, 64])\n",
      "EntropyParametersMotion.2.bias torch.Size([64])\n",
      "EntropyParametersMotion.4.weight torch.Size([2, 64])\n",
      "EntropyParametersMotion.4.bias torch.Size([2])\n",
      "EntropyFactorizedMotion.matrix torch.Size([64, 1, 3])\n",
      "EntropyFactorizedMotion.bias torch.Size([64, 1, 1])\n",
      "EntropyFactorizedMotion.factor torch.Size([64, 3, 1])\n",
      "EntropyFactorizedMotion._matrices.0 torch.Size([64, 3, 1])\n",
      "EntropyFactorizedMotion._matrices.1 torch.Size([64, 3, 3])\n",
      "EntropyFactorizedMotion._matrices.2 torch.Size([64, 3, 3])\n",
      "EntropyFactorizedMotion._matrices.3 torch.Size([64, 1, 3])\n",
      "EntropyFactorizedMotion._bias.0 torch.Size([64, 3, 1])\n",
      "EntropyFactorizedMotion._bias.1 torch.Size([64, 3, 1])\n",
      "EntropyFactorizedMotion._bias.2 torch.Size([64, 3, 1])\n",
      "EntropyFactorizedMotion._bias.3 torch.Size([64, 1, 1])\n",
      "EntropyFactorizedMotion._factor.0 torch.Size([64, 3, 1])\n",
      "EntropyFactorizedMotion._factor.1 torch.Size([64, 3, 1])\n",
      "EntropyFactorizedMotion._factor.2 torch.Size([64, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = './model.pth'\n",
    "model = torch.load(model_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fcgs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
